{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.2'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import pkg_resources\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "pkg_resources.get_distribution(\"gym\").version\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3840, 2160),\n",
       " (2560, 1600),\n",
       " (2560, 1440),\n",
       " (2048, 1536),\n",
       " (1920, 1440),\n",
       " (1920, 1200),\n",
       " (1920, 1080),\n",
       " (1768, 992),\n",
       " (1680, 1050),\n",
       " (1600, 1200),\n",
       " (1600, 1024),\n",
       " (1440, 900),\n",
       " (1366, 768),\n",
       " (1280, 1024),\n",
       " (1280, 960),\n",
       " (1280, 800),\n",
       " (1280, 720),\n",
       " (1176, 664),\n",
       " (1152, 864),\n",
       " (1024, 768),\n",
       " (800, 600),\n",
       " (720, 480),\n",
       " (640, 480)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import wandb\n",
    "\n",
    "# wandb.init(project=\"DQN_yc930401\", entity=\"xdvisch\")\n",
    "\n",
    "import pygame\n",
    "pygame.init()\n",
    "pygame.display.list_modes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xande\\AppData\\Local\\Temp\\ipykernel_1768\\3887400989.py:12: DeprecationWarning: CUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  T.Resize(40, interpolation=Image.CUBIC),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gym.wrappers.human_rendering import HumanRendering\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# env = gym.make('CartPole-v1',  render_mode='rgb_array').unwrapped\n",
    "env = gym.make('CartPole-v1',  render_mode='rgb_array')\n",
    "wrapped = HumanRendering(env)\n",
    "wrapped.reset()\n",
    "\n",
    "\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "# This is based on the code from gym.\n",
    "screen_width = 600\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.4\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cart_location():\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render().transpose((2, 0, 1))  # transpose into torch order (CHW)\n",
    "    # Strip off the top and bottom of the screen\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    \n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescare, convert to torch tensor (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        \n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    # wandb.log({\"eps_threshold\": eps_threshold})\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimize_model(policy_net, optimizer, memory):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = policy_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = ((next_state_values * GAMMA) + reward_batch).unsqueeze(1)\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the custom x axis metric\n",
    "# wandb.define_metric(\"episode\")\n",
    "\n",
    "# Define which metrics to plot against that x-axis\n",
    "# wandb.define_metric(\"duration\", step_metric='episode')\n",
    "\n",
    "def trainIters(policy_net, n_iters=60):\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "    memory = ReplayMemory(10000)\n",
    "    for iter in range(n_iters):\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        wrapped.reset()\n",
    "        print(\"spel beindigd!\")\n",
    "        # time.sleep(2)\n",
    "        last_screen = get_screen()\n",
    "        current_screen = get_screen()\n",
    "        state = current_screen - last_screen\n",
    "        losses = []\n",
    "        for t in count():\n",
    "            env.render()\n",
    "            wrapped._render_frame()\n",
    "            # print(f\"stap {t} in huidige episode\")\n",
    "            # time.sleep(5)\n",
    "            action = select_action(state)\n",
    "            \n",
    "            _, reward, done, _, _ = env.step(action.item())\n",
    "            \n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            # Observe new state\n",
    "            last_screen = current_screen\n",
    "            \n",
    "            current_screen = get_screen()\n",
    "            if not done:\n",
    "                next_state = current_screen - last_screen\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the target network)\n",
    "            optimize_model(policy_net, optimizer, memory)\n",
    "            if done:\n",
    "                \n",
    "                # log_dict = {\n",
    "                # \"episode\": iter + 1,\n",
    "                \n",
    "                # \"duration\": t\n",
    "                # }\n",
    "                # wandb.log(log_dict)\n",
    "                # print('Iteration: {}, Score: {}'.format(iter + 1, t))\n",
    "                \n",
    "                break\n",
    "\n",
    "    torch.save(policy_net, 'model/policy_net.pkl')\n",
    "    # print('Complete')\n",
    "    wrapped.render()\n",
    "    wrapped.close()\n",
    "\n",
    "    env.render()\n",
    "    env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xande\\AppData\\Local\\Temp\\ipykernel_1768\\1237586940.py:39: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:28.)\n",
      "  next_state_values[non_final_mask] = policy_net(non_final_next_states).max(1)[0].detach()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n",
      "wrapper geinitialiseerd!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\master\\masterproef\\master_thesis\\repo\\masterproef\\DQN_yc930401.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     policy_net \u001b[39m=\u001b[39m DQN()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m trainIters(policy_net, n_iters\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "\u001b[1;32md:\\master\\masterproef\\master_thesis\\repo\\masterproef\\DQN_yc930401.ipynb Cell 8\u001b[0m in \u001b[0;36mtrainIters\u001b[1;34m(policy_net, n_iters)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Perform one step of the optimization (on the target network)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m optimize_model(policy_net, optimizer, memory)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39m# log_dict = {\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m# wandb.log(log_dict)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39m# print('Iteration: {}, Score: {}'.format(iter + 1, t))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;32md:\\master\\masterproef\\master_thesis\\repo\\masterproef\\DQN_yc930401.ipynb Cell 8\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m(policy_net, optimizer, memory)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Optimize the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m policy_net\u001b[39m.\u001b[39mparameters():\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/DQN_yc930401.ipynb#X10sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     param\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mclamp_(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\xande\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\xande\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if os.path.exists('model/policy_net.pkl'):\n",
    "        policy_net = torch.load('model/policy_net.pkl')\n",
    "        print('Model loaded')\n",
    "    else:\n",
    "        policy_net = DQN().to(device)\n",
    "    trainIters(policy_net, n_iters=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b15ee443de15a7c7a9e59449ab0d06bb25873493c1d52931efe00f2e6ab94104"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
