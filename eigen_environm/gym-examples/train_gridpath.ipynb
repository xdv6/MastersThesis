{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# ! pip install gym\n",
    "# ! pip install wandb\n",
    "# ! pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import gym_game\n",
    "import pygame\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import pkg_resources\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys \n",
    "\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "from dqn_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "config =  {\n",
    "    \"BATCH_SIZE\":128,\n",
    "    \"GAMMA\" : 0.999,\n",
    "    \"EPS_START\": 1,\n",
    "    \"EPS_END\" : 0.1,\n",
    "    # \"EPS_DECAY\" : 40000,\n",
    "    \"lr\":0.0001, \n",
    "    # \"weight_decay\":1e-5,\n",
    "    # ~ number of states * 4\n",
    "    \"REPLAY_BUFFER\":1000,\n",
    "    \"EPISODES\": 1000,\n",
    "    \"TARGET_UPDATE\": 30,\n",
    "    \"SAVE_FREQ\": 10,\n",
    "    \"RESET_ENV_FREQ\": 300,\n",
    "    \"DDQN\": False,\n",
    "    \"MODEL_dir_file\": \"./model/stop_border_lagere_lr\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxdvisch\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\master\\masterproef\\master_thesis\\repo\\masterproef\\eigen_environm\\gym-examples\\wandb\\run-20230511_135505-2vytvxtf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/xdvisch/refactor/runs/2vytvxtf\" target=\"_blank\">sage-resonance-17</a></strong> to <a href=\"https://wandb.ai/xdvisch/refactor\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "run = wandb.init(project=\"refactor\", entity=\"xdvisch\", config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xande\\AppData\\Local\\Temp\\ipykernel_20260\\1505004635.py:8: DeprecationWarning: CUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  T.Resize(15, interpolation=Image.CUBIC),\n",
      "c:\\Users\\xande\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(\"GridWorld-v0\", render_mode=\"rgb_array\").unwrapped\n",
    "\n",
    "\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(15, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### steps\n",
    "```text\n",
    "    - 0: RIGHT\n",
    "    - 1: DOWN\n",
    "    - 2: LEFT\n",
    "    - 3: UP\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 15, 15])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render().transpose((2, 0, 1))  # transpose into torch order (CHW)\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "\n",
    "    # full screen\n",
    "    # screen = screen[:,:, 520:730]\n",
    "    \n",
    "    # area around agent\n",
    "    # coordinaat van linkerbovenhoek rechthoek\n",
    "    x_pixel_coo_agent = env._agent_location[0] * env.pix_square_size\n",
    "    y_pixel_coo_agent = env._agent_location[1] * env.pix_square_size\n",
    "\n",
    "    x_coo_right_up = x_pixel_coo_agent + 2 * env.pix_square_size\n",
    "    x_coo_right_down = x_pixel_coo_agent - env.pix_square_size\n",
    "\n",
    "    y_coo_left_down = y_pixel_coo_agent + 2 * env.pix_square_size\n",
    "    y_coo_left_up = y_pixel_coo_agent - env.pix_square_size\n",
    "\n",
    "    # left handed coordinate system\n",
    "    screen = screen[:,y_coo_left_up:y_coo_left_down, x_coo_right_down:x_coo_right_up]\n",
    "\n",
    "    \n",
    "\n",
    "    # Convert to float, rescare, convert to torch tensor (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "example_screen = get_screen()\n",
    "print(example_screen.shape)\n",
    "# print(f\"shape of screen: {screen.example_screen}\")\n",
    "plt.imshow(example_screen.cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# class ReplayMemory(object):\n",
    "\n",
    "#     def __init__(self, capacity):\n",
    "#         self.capacity = capacity\n",
    "        \n",
    "#         self.memory = []\n",
    "#         self.position = 0\n",
    "\n",
    "#     def push(self, *args):\n",
    "#         if len(self.memory) < self.capacity:\n",
    "#             self.memory.append(None)\n",
    "#         self.memory[self.position] = Transition(*args)\n",
    "#         self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         return random.sample(self.memory, batch_size)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.memory)\n",
    "\n",
    "\n",
    "\n",
    "# class DQN(nn.Module):\n",
    "\n",
    "#     def __init__(self, h, w, outputs):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 16, kernel_size=4, stride=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(16)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(32)\n",
    "#         self.conv3 = nn.Conv2d(32, 32, kernel_size=4, stride=1)\n",
    "#         self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "#         # Number of Linear input connections depends on output of conv2d layers\n",
    "#         # and therefore the input image size, so compute it.\n",
    "#         def conv2d_size_out(size, kernel_size = 4, stride = 1):\n",
    "#             return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "            \n",
    "#         convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w), 4, 1))\n",
    "#         convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h), 4, 1))\n",
    "#         linear_input_size = convw * convh * 32\n",
    "#         self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "#     # Called with either one element to determine next action, or a batch\n",
    "#     # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "#     def forward(self, x):\n",
    "#         x = x.to(device)\n",
    "#         x = F.relu(self.bn1(self.conv1(x)))\n",
    "#         x = F.relu(self.bn2(self.conv2(x)))\n",
    "#         x = F.relu(self.bn3(self.conv3(x)))\n",
    "#         return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps_done = -100000 * ln((0.1 - eps_threshold) / 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps_done = 0\n",
    "\n",
    "# def select_action(state):\n",
    "#     global steps_done\n",
    "#     sample = random.random()\n",
    "\n",
    "#     # steps_done = -100000 * ln((0.1 - eps_threshold) / 0.9)\n",
    "#     # eps_threshold = config.get(\"EPS_END\") + (config.get(\"EPS_START\") - config.get(\"EPS_END\")) * math.exp(-1. * steps_done / config.get(\"EPS_DECAY\"))\n",
    "#     eps_threshold = config.get(\"EPS_END\")\n",
    "\n",
    "#     wandb.log({\"eps_threshold\": eps_threshold})\n",
    "#     steps_done += 1\n",
    "#     if sample > eps_threshold:\n",
    "#         with torch.no_grad():\n",
    "#             # t.max(1) will return largest column value of each row.\n",
    "#             # second column on max result is index of where max element was\n",
    "#             # found, so we pick action with the larger expected reward.\n",
    "#             return policy_net(state).max(1)[1].view(1, 1)\n",
    "#     else:\n",
    "#         return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def optimize_model(policy_net, optimizer, memory):\n",
    "#     if len(memory) < config.get(\"BATCH_SIZE\"):\n",
    "#         return\n",
    "#     transitions = memory.sample( config.get(\"BATCH_SIZE\"))\n",
    "#     # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for detailed explanation).\n",
    "#     batch = Transition(*zip(*transitions))\n",
    "\n",
    "#     # Compute a mask of non-final states and concatenate the batch elements\n",
    "#     non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "#                                           batch.next_state)), device=device, dtype=torch.uint8)\n",
    "#     non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "#     state_batch = torch.cat(batch.state)\n",
    "#     action_batch = torch.cat(batch.action)\n",
    "#     reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "#     # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken\n",
    "    \n",
    "#     state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "#     # Compute V(s_{t+1}) for all next states.\n",
    "#     next_state_values = torch.zeros( config.get(\"BATCH_SIZE\"), device=device)\n",
    "\n",
    "#     if config.get(\"DDQN\"):\n",
    "#         ### Double DQN\n",
    "#         argmax_action = policy_net(non_final_next_states).max(1)[1].unsqueeze(1).detach()\n",
    "#         next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, argmax_action).squeeze(1).detach()\n",
    "#     else:\n",
    "#         ### DQN\n",
    "#         next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "#     # import ipdb; ipdb.set_trace()\n",
    "\n",
    "#     # Compute the expected Q values\n",
    "#     expected_state_action_values = ((next_state_values * config.get(\"GAMMA\") + reward_batch).unsqueeze(1))\n",
    "\n",
    "#     # Compute Huber loss\n",
    "#     loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "#     wandb.log({\"loss\": loss})\n",
    "#     # Optimize the model\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     for param in policy_net.parameters():\n",
    "#         param.grad.data.clamp_(-1, 1)\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.human_rendering import HumanRendering\n",
    "\n",
    "\n",
    "# Define the custom x axis metric\n",
    "wandb.define_metric(\"episode\")\n",
    "\n",
    "# Define which metrics to plot against that x-axis\n",
    "wandb.define_metric(\"reached_target\", step_metric='episode')\n",
    "wandb.define_metric(\"win_count\", step_metric='episode')\n",
    "wandb.define_metric(\"mean_reward_over_episode\", step_metric='episode')\n",
    "wandb.define_metric(\"number_of_actions_in_episode\", step_metric='episode')\n",
    "\n",
    "def trainIters(memory, policy_net, win_count, n_iters=60):\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=config.get(\n",
    "        \"lr\"))\n",
    "    \n",
    "    for iteration in range(n_iters):\n",
    "        achieved_rewards = torch.tensor([], device=device)\n",
    "        # wrapped = HumanRendering(env)\n",
    "\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        # wrapped.reset()\n",
    "\n",
    "        # state based on patch of screen (3x3 around agent)\n",
    "        state = get_screen()\n",
    "        spel_gelukt = 0\n",
    "        \n",
    "        for t in count():\n",
    "            env.render()\n",
    "            # wrapped._render_frame()\n",
    "            action = select_action(state, config, policy_net, n_actions, device)\n",
    "            _, reward, done, _, _ = env.step(action.item())\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            achieved_rewards = torch.cat((achieved_rewards, reward))\n",
    "            \n",
    "            if not done:\n",
    "                next_state = get_screen()\n",
    "\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the target network)\n",
    "            optimize_model(policy_net, optimizer, memory, config, device, target_net)\n",
    "\n",
    "            # if agent did not reach target after RESET_ENV_FREQ actions, reset environment\n",
    "            if (t + 1) % config.get(\"RESET_ENV_FREQ\") == 0:\n",
    "                done = True\n",
    "\n",
    "            if done:\n",
    "                if reward == 1000:\n",
    "                    spel_gelukt = 1\n",
    "                    win_count += 1\n",
    "\n",
    "                log_dict = {\n",
    "                    \"episode\": iteration + 1,\n",
    "                    \"reached_target\": spel_gelukt\n",
    "                }\n",
    "                wandb.log(log_dict)\n",
    "                wandb.log({\"number_of_actions_in_episode\": t})\n",
    "                wandb.log({\"win_count\": win_count})\n",
    "                wandb.log({\"mean_reward_over_episode\": torch.mean(achieved_rewards)})\n",
    "                wandb.log({\"buffer_size\": memory.__len__()})\n",
    "                break\n",
    "            \n",
    "\n",
    "        # Update the target network, copying all weights and biases to target DQN\n",
    "        if iteration % config.get(\"TARGET_UPDATE\") == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        # save model after frequency\n",
    "        # if iteration % config.get(\"SAVE_FREQ\") == 0:\n",
    "        #     torch.save(policy_net, config.get(\"MODEL_dir_file\") + str(iteration) + '.pkl')\n",
    "        \n",
    "\n",
    "    print('Complete')\n",
    "\n",
    "    env.render()\n",
    "    env.close()\n",
    "\n",
    "    # wrapped.render()\n",
    "    # wrapped.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\master\\masterproef\\master_thesis\\repo\\masterproef\\dqn_util.py:124: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:28.)\n",
      "  next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\master\\masterproef\\master_thesis\\repo\\masterproef\\eigen_environm\\gym-examples\\train_gridpath.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m target_net\u001b[39m.\u001b[39meval()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# 20 iteraties\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m trainIters(memory, policy_net, win_count, n_iters\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mEPISODES\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "\u001b[1;32md:\\master\\masterproef\\master_thesis\\repo\\masterproef\\eigen_environm\\gym-examples\\train_gridpath.ipynb Cell 12\u001b[0m in \u001b[0;36mtrainIters\u001b[1;34m(memory, policy_net, win_count, n_iters)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m achieved_rewards \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((achieved_rewards, reward))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     next_state \u001b[39m=\u001b[39m get_screen()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X14sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     next_state \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32md:\\master\\masterproef\\master_thesis\\repo\\masterproef\\eigen_environm\\gym-examples\\train_gridpath.ipynb Cell 12\u001b[0m in \u001b[0;36mget_screen\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_screen\u001b[39m():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     screen \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mrender()\u001b[39m.\u001b[39mtranspose((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))  \u001b[39m# transpose into torch order (CHW)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     _, screen_height, screen_width \u001b[39m=\u001b[39m screen\u001b[39m.\u001b[39mshape\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# full screen\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# screen = screen[:,:, 520:730]\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# area around agent\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# coordinaat van linkerbovenhoek rechthoek\u001b[39;00m\n",
      "File \u001b[1;32md:\\master\\masterproef\\master_thesis\\repo\\masterproef\\eigen_environm\\gym-examples\\gym_game\\envs\\grid_world.py:191\u001b[0m, in \u001b[0;36mGridWorldEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    190\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 191\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_frame()\n",
      "File \u001b[1;32md:\\master\\masterproef\\master_thesis\\repo\\masterproef\\eigen_environm\\gym-examples\\gym_game\\envs\\grid_world.py:202\u001b[0m, in \u001b[0;36mGridWorldEnv._render_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclock \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclock \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39mtime\u001b[39m.\u001b[39mClock()\n\u001b[1;32m--> 202\u001b[0m canvas \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m./afbeeldingen/contrast_v3.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m \u001b[39m# First we draw the target\u001b[39;00m\n\u001b[0;32m    206\u001b[0m pygame\u001b[39m.\u001b[39mdraw\u001b[39m.\u001b[39mrect(\n\u001b[0;32m    207\u001b[0m     canvas,\n\u001b[0;32m    208\u001b[0m     (\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m, \u001b[39m0\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    213\u001b[0m     ),\n\u001b[0;32m    214\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "memory = ReplayMemory(config.get(\"REPLAY_BUFFER\"))\n",
    "\n",
    "# variables for logging\n",
    "win_count = 0\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. \n",
    "\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# policy_net = torch.load('./model/gridpath_andere_afmeting_kleinere_rb.pkl')\n",
    "policy_net = DQN(screen_height, screen_width, n_actions, device, 4, 1).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions, device, 4, 1).to(device)\n",
    "wandb.watch(target_net)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# 20 iteraties\n",
    "trainIters(memory, policy_net, win_count, n_iters=config.get('EPISODES'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(policy_net, config.get(\"MODEL_dir_file\") + 'end.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b15ee443de15a7c7a9e59449ab0d06bb25873493c1d52931efe00f2e6ab94104"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
