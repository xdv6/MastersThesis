{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import gym_game\n",
    "import pygame\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import pkg_resources\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pkg_resources.get_distribution(\"gym\").version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "config =  {\n",
    "    \"BATCH_SIZE\":32,\n",
    "    \"GAMMA\" : 0.95,\n",
    "    \"EPS_START\": 1,\n",
    "    \"EPS_END\" : 0.1,\n",
    "    \"EPS_DECAY\" : 200,\n",
    "    \"lr\":0.001, \n",
    "    # \"weight_decay\":1e-5,\n",
    "    # ~ number of states * 4\n",
    "    \"REPLAY_BUFFER\":10000,\n",
    "    \"EPISODES\": 2000,\n",
    "    \"TARGET_UPDATE\": 1000,\n",
    "    \"SAVE_FREQ\": 10,\n",
    "    \"RESET_ENV_FREQ\": 300\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01f8055305a43fb8d4eb1ac2684cb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.033419259389241535, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\master\\masterproef\\master_thesis\\repo\\masterproef\\eigen_environm\\gym-examples\\wandb\\run-20230419_100614-xu8hohb1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/xdvisch/higher_constrast_downscaled/runs/xu8hohb1\" target=\"_blank\">scarlet-mountain-2</a></strong> to <a href=\"https://wandb.ai/xdvisch/higher_constrast_downscaled\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "run = wandb.init(project=\"y-reward\", entity=\"xdvisch\", config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xande\\AppData\\Local\\Temp\\ipykernel_12088\\1256961729.py:8: DeprecationWarning: CUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  T.Resize(10, interpolation=Image.CUBIC),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(\"GridWorld-v0\", render_mode=\"rgb_array\").unwrapped\n",
    "\n",
    "\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(10, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### steps\n",
    "```text\n",
    "    - 0: RIGHT\n",
    "    - 1: DOWN\n",
    "    - 2: LEFT\n",
    "    - 3: UP\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render().transpose((2, 0, 1))  # transpose into torch order (CHW)\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "\n",
    "    # full screen\n",
    "    # screen = screen[:,:, 520:730]\n",
    "    \n",
    "    # area around agent\n",
    "    # coordinaat van linkerbovenhoek rechthoek\n",
    "    x_pixel_coo_agent = env._agent_location[0] * env.pix_square_size\n",
    "    y_pixel_coo_agent = env._agent_location[1] * env.pix_square_size\n",
    "\n",
    "    x_coo_right_up = x_pixel_coo_agent + 2 * env.pix_square_size\n",
    "    x_coo_right_down = x_pixel_coo_agent - env.pix_square_size\n",
    "\n",
    "    y_coo_left_down = y_pixel_coo_agent + 2 * env.pix_square_size\n",
    "    y_coo_left_up = y_pixel_coo_agent - env.pix_square_size\n",
    "\n",
    "    # left handed coordinate system\n",
    "    screen = screen[:,y_coo_left_up:y_coo_left_down, x_coo_right_down:x_coo_right_up]\n",
    "\n",
    "    \n",
    "\n",
    "    # Convert to float, rescare, convert to torch tensor (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "example_screen = get_screen()\n",
    "# print(f\"shape of screen: {screen.example_screen}\")\n",
    "plt.imshow(example_screen.cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        \n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 3, stride = 1):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO double dqn\n",
    "https://datascience.stackexchange.com/questions/32246/q-learning-target-network-vs-double-dqn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extra uitleg detach\n",
    "\n",
    "Ja, dat klopt. De detach() methode zorgt ervoor dat de tensor waarop het wordt aangeroepen wordt losgekoppeld van de computationele grafiek. Dit betekent dat het niet meer wordt beschouwd als onderdeel van de grafiek en dus niet meer wordt meegenomen in de berekeningen van de backward pass.\n",
    "\n",
    "In het gegeven codefragment wordt detach() gebruikt om de tensor die wordt berekend met target_net(non_final_next_states).max(1)[0] los te koppelen van de computationele grafiek. Hierdoor zorgt het ervoor dat de gradienten van de next_state_values tensor niet worden beïnvloed door de gradienten die worden berekend door het target_net netwerk. Dit kan wenselijk zijn in bepaalde situaties, bijvoorbeeld als je niet wilt dat de parameters van het target_net netwerk worden geüpdatet door de backward pass van de next_state_values tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "\n",
    "\n",
    "    # eps_threshold = config.get(\"EPS_END\") + (config.get(\"EPS_START\") - config.get(\"EPS_END\")) * math.exp(-1. * steps_done / config.get(\"EPS_DECAY\"))\n",
    "    eps_threshold = config.get(\"EPS_END\")\n",
    "\n",
    "    wandb.log({\"eps_threshold\": eps_threshold})\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimize_model(policy_net, optimizer, memory):\n",
    "    if len(memory) < config.get(\"BATCH_SIZE\"):\n",
    "        return\n",
    "    transitions = memory.sample( config.get(\"BATCH_SIZE\"))\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken\n",
    "    # na policy_net heb je: [[leftexp,rightexp,upexp,downexp], [left,...] ..] dan pak je met gather telkens actie vanuit de batch via axis 1 \n",
    "    # dan heb je als batch bv (left, right) is => [[leftexp], [rightexp]]\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros( config.get(\"BATCH_SIZE\"), device=device)\n",
    "    # max(1) returns largest column value of each row met index erbij\n",
    "    # [0] returns the max value of each row (we only want the max value, not the index)\n",
    "    # detach to obtain the output of a neural network and apply it to a downstream task without updating the network parameters with backpropagation\n",
    "    # detach maakt copy van tensor zodat operaties niet in de backpropagation berekeningen komen en paramters van target_net niet beinvloed worden. zie extra uitleg cell hierboven\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = ((next_state_values * config.get(\"GAMMA\") + reward_batch).unsqueeze(1))\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    wandb.log({\"loss\": loss})\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.human_rendering import HumanRendering\n",
    "\n",
    "\n",
    "# Define the custom x axis metric\n",
    "wandb.define_metric(\"episode\")\n",
    "\n",
    "# Define which metrics to plot against that x-axis\n",
    "wandb.define_metric(\"reached_target\", step_metric='episode')\n",
    "wandb.define_metric(\"win_count\", step_metric='episode')\n",
    "wandb.define_metric(\"mean_reward\", step_metric='episode')\n",
    "wandb.define_metric(\"number_of_actions_in_episode\", step_metric='episode')\n",
    "\n",
    "def trainIters(policy_net, achieved_rewards,running_sum, counter, win_count, n_iters=60):\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=config.get(\n",
    "        \"lr\"))\n",
    "    memory = ReplayMemory(config.get(\"REPLAY_BUFFER\"))\n",
    "    for iteration in range(n_iters):\n",
    "\n",
    "        # wrapped = HumanRendering(env)\n",
    "\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        # wrapped.reset()\n",
    "\n",
    "        # state based on patch of screen (3x3 around agent)\n",
    "        state = get_screen()\n",
    "        spel_gelukt = 0\n",
    "        \n",
    "        for t in count():\n",
    "            env.render()\n",
    "            # wrapped._render_frame()\n",
    "            action = select_action(state)\n",
    "            _, reward, done, _, _ = env.step(action.item())\n",
    "            \n",
    "            running_sum += reward\n",
    "            counter += 1\n",
    "            mean = running_sum / counter\n",
    "\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            \n",
    "            if not done:\n",
    "                next_state = get_screen()\n",
    "\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the target network)\n",
    "            optimize_model(policy_net, optimizer, memory)\n",
    "\n",
    "            # if agent did not reach target after RESET_ENV_FREQ actions, reset environment\n",
    "            if (t + 1) % config.get(\"RESET_ENV_FREQ\") == 0:\n",
    "                done = True\n",
    "\n",
    "            if done:\n",
    "                if reward == 1000:\n",
    "                    spel_gelukt = 1\n",
    "                    win_count += 1\n",
    "\n",
    "                log_dict = {\n",
    "                    \"episode\": iteration + 1,\n",
    "                    \"reached_target\": spel_gelukt\n",
    "                }\n",
    "                wandb.log(log_dict)\n",
    "                wandb.log({\"number_of_actions_in_episode\": t})\n",
    "                wandb.log({\"win_count\": win_count})\n",
    "                wandb.log({\"mean_reward\": mean})\n",
    "                break\n",
    "            \n",
    "\n",
    "        # Update the target network, copying all weights and biases to target DQN\n",
    "        if iteration % config.get(\"TARGET_UPDATE\") == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        # save model after frequency\n",
    "        if iteration % config.get(\"SAVE_FREQ\") == 0:\n",
    "            torch.save(policy_net, './model/y-reward_' + str(iteration) + '.pkl')\n",
    "        \n",
    "\n",
    "    print('Complete')\n",
    "\n",
    "    env.render()\n",
    "    env.close()\n",
    "\n",
    "    # wrapped.render()\n",
    "    # wrapped.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xande\\AppData\\Local\\Temp\\ipykernel_12088\\2887323251.py:45: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:28.)\n",
      "  next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\master\\masterproef\\master_thesis\\repo\\masterproef\\eigen_environm\\gym-examples\\train_gridpath.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m target_net\u001b[39m.\u001b[39meval()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# 20 iteraties\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m trainIters(policy_net, achieved_rewards, running_sum, counter, win_count, n_iters\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mEPISODES\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "\u001b[1;32md:\\master\\masterproef\\master_thesis\\repo\\masterproef\\eigen_environm\\gym-examples\\train_gridpath.ipynb Cell 11\u001b[0m in \u001b[0;36mtrainIters\u001b[1;34m(policy_net, achieved_rewards, running_sum, counter, win_count, n_iters)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m spel_gelukt \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m count():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     env\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m# wrapped._render_frame()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/master/masterproef/master_thesis/repo/masterproef/eigen_environm/gym-examples/train_gridpath.ipynb#X13sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     action \u001b[39m=\u001b[39m select_action(state)\n",
      "File \u001b[1;32md:\\master\\masterproef\\master_thesis\\repo\\masterproef\\eigen_environm\\gym-examples\\gym_game\\envs\\grid_world.py:189\u001b[0m, in \u001b[0;36mGridWorldEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    188\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 189\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_frame()\n",
      "File \u001b[1;32md:\\master\\masterproef\\master_thesis\\repo\\masterproef\\eigen_environm\\gym-examples\\gym_game\\envs\\grid_world.py:200\u001b[0m, in \u001b[0;36mGridWorldEnv._render_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclock \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclock \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39mtime\u001b[39m.\u001b[39mClock()\n\u001b[1;32m--> 200\u001b[0m canvas \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m./afbeeldingen/street_high_contrast.png\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    203\u001b[0m \u001b[39m# First we draw the target\u001b[39;00m\n\u001b[0;32m    204\u001b[0m pygame\u001b[39m.\u001b[39mdraw\u001b[39m.\u001b[39mrect(\n\u001b[0;32m    205\u001b[0m     canvas,\n\u001b[0;32m    206\u001b[0m     (\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m, \u001b[39m0\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m     ),\n\u001b[0;32m    212\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # variables for logging\n",
    "    win_count = 0\n",
    "    achieved_rewards = torch.tensor([], device=device)\n",
    "    running_sum = achieved_rewards.sum()\n",
    "    counter = achieved_rewards.numel()\n",
    "\n",
    "    # Get screen size so that we can initialize layers correctly based on shape\n",
    "    # returned from AI gym. \n",
    "\n",
    "    init_screen = get_screen()\n",
    "    _, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "    # Get number of actions from gym action space\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    # policy_net = torch.load('./model/gridpath_andere_afmeting_kleinere_rb.pkl')\n",
    "    policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "    target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    # 20 iteraties\n",
    "    trainIters(policy_net, achieved_rewards, running_sum, counter, win_count, n_iters=config.get('EPISODES'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(policy_net, './model/y-reward.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b15ee443de15a7c7a9e59449ab0d06bb25873493c1d52931efe00f2e6ab94104"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
