{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import pkg_resources\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "pkg_resources.get_distribution(\"gym\").version\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1920, 1080),\n",
       " (1680, 1050),\n",
       " (1600, 900),\n",
       " (1440, 900),\n",
       " (1400, 1050),\n",
       " (1366, 768),\n",
       " (1360, 768),\n",
       " (1280, 1024),\n",
       " (1280, 960),\n",
       " (1280, 800),\n",
       " (1280, 768),\n",
       " (1280, 720),\n",
       " (1280, 600),\n",
       " (1152, 864),\n",
       " (1024, 768),\n",
       " (800, 600),\n",
       " (640, 480),\n",
       " (640, 400),\n",
       " (512, 384),\n",
       " (400, 300),\n",
       " (320, 240),\n",
       " (320, 200)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import wandb\n",
    "\n",
    "# wandb.init(project=\"DQN_yc930401\", entity=\"xdvisch\")\n",
    "\n",
    "import pygame\n",
    "pygame.init()\n",
    "pygame.display.list_modes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xande\\AppData\\Local\\Temp\\ipykernel_11292\\3887400989.py:12: DeprecationWarning: CUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  T.Resize(40, interpolation=Image.CUBIC),\n",
      "c:\\Users\\xande\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gym.wrappers.human_rendering import HumanRendering\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# env = gym.make('CartPole-v1',  render_mode='rgb_array').unwrapped\n",
    "env = gym.make('CartPole-v1',  render_mode='rgb_array')\n",
    "wrapped = HumanRendering(env)\n",
    "wrapped.reset()\n",
    "\n",
    "\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "# This is based on the code from gym.\n",
    "screen_width = 600\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.4\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cart_location():\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render().transpose((2, 0, 1))  # transpose into torch order (CHW)\n",
    "    # Strip off the top and bottom of the screen\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    \n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescare, convert to torch tensor (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        \n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "\n",
    "    # eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    eps_threshold = 0\n",
    "\n",
    "\n",
    "    # wandb.log({\"eps_threshold\": eps_threshold})\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimize_model(policy_net, optimizer, memory):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = policy_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = ((next_state_values * GAMMA) + reward_batch).unsqueeze(1)\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_treshold_range = (-2.4, 2.4)\n",
    "# pole_treshold_range = (-.2095, .2095)\n",
    "\n",
    "# Define the custom x axis metric\n",
    "# wandb.define_metric(\"episode\")\n",
    "\n",
    "# Define which metrics to plot against that x-axis\n",
    "# wandb.define_metric(\"duration\", step_metric='episode')\n",
    "\n",
    "def trainIters(policy_net, n_iters=60):\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "    memory = ReplayMemory(10000)\n",
    "    for iter in range(n_iters):\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        wrapped.reset()\n",
    "        \n",
    "        # time.sleep(2)\n",
    "        last_screen = get_screen()\n",
    "        current_screen = get_screen()\n",
    "        state = current_screen - last_screen\n",
    "        losses = []\n",
    "        for t in count():\n",
    "            env.render()\n",
    "            wrapped._render_frame()\n",
    "            # print(f\"stap {t} in huidige episode\")\n",
    "            # time.sleep(5)\n",
    "            action = select_action(state)\n",
    "            value, reward, done, _, _ = env.step(action.item())\n",
    "            \n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            # Observe new state\n",
    "            last_screen = current_screen\n",
    "            \n",
    "            current_screen = get_screen()\n",
    "            if not done:\n",
    "                next_state = current_screen - last_screen\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the target network)\n",
    "            # optimize_model(policy_net, optimizer, memory)\n",
    "            if done:\n",
    "                print(f\"spel beindigd, x_co:{value[0]} en hoek:{value[2]}\")\n",
    "                # log_dict = {\n",
    "                # \"episode\": iter + 1,\n",
    "                \n",
    "                # \"duration\": t\n",
    "                # }\n",
    "                # wandb.log(log_dict)\n",
    "                # print('Iteration: {}, Score: {}'.format(iter + 1, t))\n",
    "                \n",
    "                break\n",
    "\n",
    "    torch.save(policy_net, 'model/policy_net.pkl')\n",
    "    # print('Complete')\n",
    "    wrapped.render()\n",
    "    wrapped.close()\n",
    "\n",
    "    env.render()\n",
    "    env.close()\n",
    "    print('treshold staat op 0 en optimize model staat af!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "spel beindigd, x_co:0.07043857127428055 en hoek:0.2147693634033203\n",
      "spel beindigd, x_co:0.15730102360248566 en hoek:0.22547738254070282\n",
      "spel beindigd, x_co:-0.014188811182975769 en hoek:0.23027750849723816\n",
      "spel beindigd, x_co:-0.046308279037475586 en hoek:0.2322358340024948\n",
      "spel beindigd, x_co:-0.1594269871711731 en hoek:0.21884307265281677\n",
      "spel beindigd, x_co:0.03353196382522583 en hoek:0.21765626966953278\n",
      "spel beindigd, x_co:0.511530876159668 en hoek:0.2237391322851181\n",
      "spel beindigd, x_co:-0.04390084370970726 en hoek:0.22311511635780334\n",
      "spel beindigd, x_co:0.05609886720776558 en hoek:0.23450630903244019\n",
      "spel beindigd, x_co:0.04089672118425369 en hoek:0.2325468510389328\n",
      "spel beindigd, x_co:-0.14553110301494598 en hoek:0.21232371032238007\n",
      "spel beindigd, x_co:-0.10705836117267609 en hoek:0.22050096094608307\n",
      "spel beindigd, x_co:-0.17061357200145721 en hoek:0.2360360324382782\n",
      "spel beindigd, x_co:-0.005341177806258202 en hoek:0.2109822928905487\n",
      "spel beindigd, x_co:-0.07674867659807205 en hoek:0.21248900890350342\n",
      "spel beindigd, x_co:-0.16780920326709747 en hoek:0.2260512262582779\n",
      "spel beindigd, x_co:-0.03155794367194176 en hoek:0.2335227131843567\n",
      "spel beindigd, x_co:-0.0271185040473938 en hoek:0.22792337834835052\n",
      "spel beindigd, x_co:-0.1371215283870697 en hoek:0.22595322132110596\n",
      "spel beindigd, x_co:-0.08124008774757385 en hoek:0.225935697555542\n",
      "spel beindigd, x_co:-0.03964496776461601 en hoek:0.22170129418373108\n",
      "spel beindigd, x_co:-0.07384781539440155 en hoek:0.22601453959941864\n",
      "spel beindigd, x_co:-0.07561645656824112 en hoek:0.21845370531082153\n",
      "spel beindigd, x_co:-0.004638075828552246 en hoek:0.21317613124847412\n",
      "spel beindigd, x_co:0.08740164339542389 en hoek:0.2274658977985382\n",
      "spel beindigd, x_co:-0.07992752641439438 en hoek:0.21419212222099304\n",
      "spel beindigd, x_co:0.01670839823782444 en hoek:0.2202211171388626\n",
      "spel beindigd, x_co:-0.0819842591881752 en hoek:0.21116527915000916\n",
      "spel beindigd, x_co:0.1461426168680191 en hoek:0.2109263837337494\n",
      "spel beindigd, x_co:-0.044286709278821945 en hoek:0.21814125776290894\n",
      "spel beindigd, x_co:-0.05375364050269127 en hoek:0.23433910310268402\n",
      "spel beindigd, x_co:0.07588375359773636 en hoek:0.21154408156871796\n",
      "spel beindigd, x_co:-0.020947851240634918 en hoek:0.23259103298187256\n",
      "spel beindigd, x_co:-0.05758396536111832 en hoek:0.2198026478290558\n",
      "spel beindigd, x_co:0.08747217804193497 en hoek:0.2169313281774521\n",
      "spel beindigd, x_co:0.2548640966415405 en hoek:0.22240187227725983\n",
      "spel beindigd, x_co:-0.11586186289787292 en hoek:0.22564977407455444\n",
      "spel beindigd, x_co:0.0961512103676796 en hoek:0.22271043062210083\n",
      "spel beindigd, x_co:0.08867523819208145 en hoek:0.2207627296447754\n",
      "spel beindigd, x_co:0.017622554674744606 en hoek:0.21347880363464355\n",
      "spel beindigd, x_co:0.5606913566589355 en hoek:0.23280631005764008\n",
      "spel beindigd, x_co:0.16784900426864624 en hoek:0.22582004964351654\n",
      "spel beindigd, x_co:-0.05089802294969559 en hoek:0.2274152934551239\n",
      "spel beindigd, x_co:0.029870597645640373 en hoek:0.2292572408914566\n",
      "spel beindigd, x_co:-0.07365899533033371 en hoek:0.21391090750694275\n",
      "spel beindigd, x_co:-0.05203680321574211 en hoek:0.22240351140499115\n",
      "spel beindigd, x_co:0.4241613447666168 en hoek:0.21305353939533234\n",
      "spel beindigd, x_co:0.04482370987534523 en hoek:0.2167239785194397\n",
      "spel beindigd, x_co:-0.04849502816796303 en hoek:0.22973774373531342\n",
      "spel beindigd, x_co:-0.07771538197994232 en hoek:0.2112494856119156\n",
      "treshold staat op 0 en optimize model staat af!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if os.path.exists('model/policy_net.pkl'):\n",
    "        policy_net = torch.load('model/policy_net.pkl')\n",
    "        print('Model loaded')\n",
    "        \n",
    "    else:\n",
    "        policy_net = DQN().to(device)\n",
    "    trainIters(policy_net, n_iters=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b15ee443de15a7c7a9e59449ab0d06bb25873493c1d52931efe00f2e6ab94104"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
